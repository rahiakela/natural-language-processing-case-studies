{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text-generation-using-gpt.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO5fcUBTZ41WaJLtekkvvm+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/natural-language-processing-case-studies/blob/master/gpt-mechanism/text_generation_using_gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c15Osm4ny2c"
      },
      "source": [
        "## Text generation using GPT-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnGMDYZtnzK-"
      },
      "source": [
        "Transformers can be pretrained on large bodies of unlabeled data and\n",
        "then finetuned for other tasks. Two main groups of such pretrained models are:\n",
        "\n",
        "1. **Bidirectional Encoder Representations from Transformers (BERTs)**\n",
        "2. **Generative Pretrained Transformers (GPTs)**\n",
        "\n",
        "The first GPT model was introduced in a 2018 paper by Radford et al. from OpenAI – it demonstrated how a generative language model can acquire knowledge and process longrange dependencies thanks to pretraining on a large, diverse corpus of contiguous text. Two successor models (trained on more extensive corpora) were released in the following years: GPT-2 in 2019 (1.5 billion parameters) and GPT-3 in 2020 (175 billion parameters).\n",
        "\n",
        "We will be making use of the excellent Transformers library created by Hugging Face(https://huggingface.co/). It abstracts away several components of the building process, allowing us to focus on the model performance and intended performance.\n",
        "\n",
        "Reference:\n",
        "\n",
        "1. https://huggingface.co/blog/how-to-generate\n",
        "2. https://huggingface.co/transformers/model_doc/gpt2.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1_hJjr6ovDC"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZiLOM67pdXZ"
      },
      "source": [
        "# installing Transformers and TensorFlow 2.0 in one line\n",
        "!pip install transformers[tf-gpu]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sntym55rowLK"
      },
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B3_LDHkpA_B"
      },
      "source": [
        "One of the advantages of the Transformers library – and a reason for its popularity, undoubtedly – is how easily we can download a specific model (and also define the appropriate tokenizer):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCq8SlkJo2Gl"
      },
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
        "GPT2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2-large\", pad_token_id=tokenizer.eos_token_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNtNvW_YqjP1"
      },
      "source": [
        "It is usually a good idea to fix the random seed to ensure the results are reproducible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aB0TiWosqZqJ"
      },
      "source": [
        "# settings\n",
        "\n",
        "# for reproducability\n",
        "SEED=34\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# maximum number of words in output text\n",
        "MAX_LEN = 70"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pfz83P6Ksigt"
      },
      "source": [
        "## Different Decoding Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVTm0hAssNHA"
      },
      "source": [
        "Let us focus on the fact that how we decode is one of the most important decisions when using a GPT-2 model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viqlesHjsohI"
      },
      "source": [
        "### Greedy search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2dOd-bjspsz"
      },
      "source": [
        "With **greedy search**, the word with the highest probability is predicted as the next word in the sequence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxoRlBihsApD"
      },
      "source": [
        "input_sequence1 = \"I don't know about you, but there's only one thing I want to do after a long day of work\"\n",
        "input_sequence2 = \"There are times when I am really tired of people, but I feel lonely too.\""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmyndH34s9Hq"
      },
      "source": [
        "Once we have our input sequence, we encode it and then call a decode method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8QvN71ls9vZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfd17355-7b38-4f42-c308-a35722cb7ac1"
      },
      "source": [
        "# encode context the generation is conditioned on\n",
        "input_ids = tokenizer.encode(input_sequence1, return_tensors=\"tf\")\n",
        "\n",
        "# generate text until the output length (which includes the context length) reaches 70\n",
        "greedy_output = GPT2.generate(input_ids, max_length=MAX_LEN)\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I don't know about you, but there's only one thing I want to do after a long day of work: go to the gym.\n",
            "\n",
            "I'm not talking about the gym that's right next to my house. I'm talking about the gym that's right next to my house.\n",
            "\n",
            "I'm not talking about the gym that\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBbXMWCxVOqS",
        "outputId": "0450477e-1e5e-4215-f193-fcba1db82234",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# encode context the generation is conditioned on\n",
        "input_ids = tokenizer.encode(input_sequence2, return_tensors=\"tf\")\n",
        "\n",
        "# generate text until the output length (which includes the context length) reaches 70\n",
        "greedy_output = GPT2.generate(input_ids, max_length=MAX_LEN)\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "There are times when I am really tired of people, but I feel lonely too. I feel like I'm alone in the world. I feel like I'm alone in my own body. I feel like I'm alone in my own mind. I feel like I'm alone in my own heart. I feel like I'm alone in my own mind\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5KpY9O3t5XQ"
      },
      "source": [
        "As you can see, the results leave some room for improvement: the model starts repeating itself, because the high-probability words mask the less-likely ones so they cannot explore more diverse combinations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6-Jti5374Rx"
      },
      "source": [
        "### Beam search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63E3eJJn75Rr"
      },
      "source": [
        "A simple remedy is **beam search**: we keep track of the alternative variants, so that more comparisons are possible:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIswnzp7ttrQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "036ae2f2-fa18-41d0-c0e2-975f0ee3af11"
      },
      "source": [
        "# set return_num_sequences > 1\n",
        "beam_outputs = GPT2.generate(input_ids, max_length=MAX_LEN, num_beams=5, no_repeat_ngram_size=2, num_return_sequences=5, early_stopping=True)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "\n",
        "# now we have 5 output sequences\n",
        "for i, beam_output in enumerate(beam_outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: There are times when I am really tired of people, but I feel lonely too. I don't know what to do with myself.\"\n",
            "\n",
            "\"I feel like I can't do anything right now,\" she said. \"I'm so tired.\"\n",
            "1: There are times when I am really tired of people, but I feel lonely too. I don't know what to do with myself.\"\n",
            "\n",
            "\"I feel like I can't do anything right now,\" she says. \"I'm so tired.\"\n",
            "2: There are times when I am really tired of people, but I feel lonely too. I don't know what to do with myself.\"\n",
            "\n",
            "\"I feel like I can't do anything right now,\" she says. \"I'm not sure what I'm supposed to be doing with my life.\"\n",
            "3: There are times when I am really tired of people, but I feel lonely too. I don't know what to do with myself.\"\n",
            "\n",
            "\"I feel like I can't do anything right now,\" she says. \"I'm not sure what I'm supposed to be doing.\"\n",
            "4: There are times when I am really tired of people, but I feel lonely too. I don't know what to do with myself.\"\n",
            "\n",
            "\"I feel like I can't do anything right now,\" she says. \"I'm not sure what I'm supposed to be doing with my life, or if I even have a life.\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YHPKwDu9cmR"
      },
      "source": [
        "This is definitely more diverse – the message is the same, but at least the formulations look a little different from a style point of view."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFOjsyxwasiG"
      },
      "source": [
        "## Sampling – Indeterministic Decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH4Bq5wSawNq"
      },
      "source": [
        "we can explore sampling – indeterministic decoding. Instead of following a strict path to find the end text with the highest probability, we rather randomly pick the next word by its conditional probability distribution. \n",
        "\n",
        "This approach risks producing incoherent ramblings, so we make use of the temperature parameter, which affects the probability mass distribution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9djkZbOs8wTh",
        "outputId": "3c03e4b7-46ad-4f77-e2e3-d2c3681cd6c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# use temperature to decrease the sensitivity to low probability candidates\n",
        "sample_output = GPT2.generate(input_ids, do_sample=True, max_length=MAX_LEN, top_k=0, temperature=0.2)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "There are times when I am really tired of people, but I feel lonely too. I feel like I'm alone in my own world. I feel like I'm alone in my own life. I feel like I'm alone in my own mind. I feel like I'm alone in my own heart. I feel like I'm alone in my own\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZB7hr3n_cctR"
      },
      "source": [
        "What happens if we increase the temperature?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lL5D5bS9cVk9",
        "outputId": "0d1b2803-9d34-4cf4-d45e-eaa5ae79b962",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# use temperature to decrease the sensitivity to low probability candidates\n",
        "sample_output = GPT2.generate(input_ids, do_sample=True, max_length=MAX_LEN, top_k=0, temperature=0.8)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "There are times when I am really tired of people, but I feel lonely too. I find it strange how the people around me seem to be always so nice. The only time I feel lonely is when I'm on the road. I can't be alone with my thoughts.\n",
            "\n",
            "What are some of your favourite things to do in the area\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SS5x1pzc021"
      },
      "source": [
        "This is getting more interesting, although it still feels a bit like a train of thought – which is perhaps to be expected, given the content of our prompt. Let's explore some more ways to tune the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rB338GqIc4x4"
      },
      "source": [
        "### Top-K sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnoiMI34c6Ce"
      },
      "source": [
        "In **Top-K sampling**, the top k most likely next words are selected and the entire probability mass is shifted to these k words. So instead of increasing the chances of high-probability words occurring and decreasing the chances of low-probability words, we just remove lowprobability words altogether."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Amr6ezvZciWq",
        "outputId": "7159e2f9-c74a-4cb4-f2cd-ecb4620ad385",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# sample from only top_k most likely words\n",
        "sample_output = GPT2.generate(input_ids, do_sample=True, max_length=MAX_LEN, top_k=50)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "There are times when I am really tired of people, but I feel lonely too. I go to a place where you can feel comfortable. It's a place where you can relax. But if you're so tired of going along with the rules, maybe I won't go. You know what? Maybe if I don't go, you won't\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLa8DI5gdhbk"
      },
      "source": [
        "This seems like a step in the right direction. Can we do better?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwzOLWnkdh73"
      },
      "source": [
        "###Top-P sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODioxA3odkc1"
      },
      "source": [
        "**Top-P sampling** (also known as nucleus sampling) is similar to Top-K, but instead of choosing the top k most likely words, we choose the smallest set of words whose total probability is larger than p, and then the entire probability mass is shifted to the words in this set. \n",
        "\n",
        "The main difference here is that with Top-K sampling, the size of the set of words is static (obviously), whereas in Top-P sampling, the size of the set can change. \n",
        "\n",
        "To use this sampling method, we just set top_k = 0 and choose a top_p value:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksKu7fz4eAvs",
        "outputId": "40156355-ed30-4e65-b9ba-c83ce93ea103",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# sample only from 80% most likely words\n",
        "sample_output = GPT2.generate(input_ids, do_sample=True, max_length=MAX_LEN, top_k=0, top_p=0.8)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "There are times when I am really tired of people, but I feel lonely too. I feel like I should just be standing there, just sitting there. I know I'm not a danger to anybody. I just feel alone.\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3etz7P40eTyN"
      },
      "source": [
        "We can combine both approaches:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpONJXHveK_m",
        "outputId": "2f4c68f3-a67a-45b7-c670-3179177ed244",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# combine both sampling techniques\n",
        "sample_outputs = GPT2.generate(input_ids, do_sample=True, max_length= 2* MAX_LEN, top_k=50, top_p=0.85, num_return_sequences=5)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
        "  print(\"\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: There are times when I am really tired of people, but I feel lonely too. I don't feel like I am being respected by my own country, which is why I am trying to change the government.\"\n",
            "\n",
            "In a recent video posted to YouTube, Mr. Jaleel, dressed in a suit and tie, talks about his life in Pakistan and his frustration at his treatment by the country's law enforcement agencies. He also describes how he met a young woman from California who helped him organize the protest in Washington.\n",
            "\n",
            "\"She was a journalist who worked with a television channel in Pakistan,\" Mr. Jaleel says in the video. \"She came to my home one day,\n",
            "\n",
            "1: There are times when I am really tired of people, but I feel lonely too. It's not that I don't like to be around other people, but it's just something I have to face sometimes.\n",
            "\n",
            "What is your favorite thing to eat?\n",
            "\n",
            "The most favorite thing I have eaten is chicken and waffles. But I love rice, soups, and even noodles. I also like to eat bread, but I like it a little bit less.\n",
            "\n",
            "What is your ideal day of eating?\n",
            "\n",
            "It varies every day. Sometimes I want to eat at home, because I'm in a house with my family. But then sometimes I just have to have some sort\n",
            "\n",
            "2: There are times when I am really tired of people, but I feel lonely too. I think that there is something in my heart that is trying to be a better person, but I don't know what that is.\"\n",
            "\n",
            "So what can be done?\n",
            "\n",
            "\"I want people to take the time to think about this,\" says Jorja, who lives in a small town outside of Boston.\n",
            "\n",
            "\"I'm not a religious person,\" she says. \"I just want people to stop judging me. People judge me for doing what I'm doing, and they are very judgmental, and I don't like it.\"\n",
            "\n",
            "This article was first published on The Conversation.\n",
            "\n",
            "3: There are times when I am really tired of people, but I feel lonely too.\n",
            "\n",
            "I want to be able to take good care of myself. I am going to be a very good person, even if I am lonely.\n",
            "\n",
            "I want to be able to take good care of my family, too. If I had a child with my wife, she would be my only daughter.\n",
            "\n",
            "I don't have the means, but I hope I will be able to do something good. I want to become good.\n",
            "\n",
            "I'm not bad at sports. I can play basketball and baseball, and even soccer.\n",
            "\n",
            "I don't have much money, but I want to\n",
            "\n",
            "4: There are times when I am really tired of people, but I feel lonely too. The only person I really love is my family. It's just that I'm not alone.\"\n",
            "\n",
            "-Juan, 24, a student\n",
            "\n",
            "A study from the European Economic Area, a free trade area between the EU and Iceland, showed that there are 2.3 million EU citizens living in Iceland. Another survey in 2014 showed that 1.3 million people in Iceland were employed.\n",
            "\n",
            "The government is committed to making Iceland a country where everyone can live and work.\n",
            "\n",
            "\"We are here to help, not to steal,\" said one of the people who drove up in a Volkswagen.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDzu1Pcze410"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}