{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3-language-modeling.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOpIptDRmOYKR7T8/WYo6SE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7ca54a165f4f4ed79aad5d22ef96fb77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_629b4d7a6be34ae78460ac2f32480d24",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b6bd0d06bb3e46dc913f7b065bb851a9",
              "IPY_MODEL_4b6bdf495be941ca9acfdedad9fa7208"
            ]
          }
        },
        "629b4d7a6be34ae78460ac2f32480d24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b6bd0d06bb3e46dc913f7b065bb851a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d7fb7f4be56a44328cba826be711cf76",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1340675298,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1340675298,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8f8d6403f0634504a86002bb99147c4e"
          }
        },
        "4b6bdf495be941ca9acfdedad9fa7208": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cb15721da81c44228ef606e212209373",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.34G/1.34G [00:38&lt;00:00, 34.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1afcee0e45514679be479275b8e38423"
          }
        },
        "d7fb7f4be56a44328cba826be711cf76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8f8d6403f0634504a86002bb99147c4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cb15721da81c44228ef606e212209373": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1afcee0e45514679be479275b8e38423": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/natural-language-processing-case-studies/blob/master/huggingface-transformers-practice/transformers-use-cases/3_language_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jivRMocQdzfI"
      },
      "source": [
        "## Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9Pb8l0Si-dN"
      },
      "source": [
        "Language modeling is the task of fitting a model to a corpus, which can be domain specific. All popular transformer-based models are trained using a variant of language modeling, e.g. \n",
        "\n",
        "- **BERT with masked language modeling**\n",
        "- **GPT-2 with causal language modeling**\n",
        "\n",
        "Language modeling can be useful outside of pretraining as well, for example to shift the model distribution to be domain-specific: using a language model trained over a very large corpus, and then fine-tuning it to a news dataset or on scientific papers e.g. [LysandreJik/arxiv-nlp](https://huggingface.co/lysandre/arxiv-nlp).\n",
        "\n",
        "Referemce: https://huggingface.co/transformers/task_summary.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7PywmnSL_uC"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYm8-n_bMelQ"
      },
      "source": [
        "%tensorflow_version 2.x     # magic command instructing to use TensorFlow version 2+\n",
        "import tensorflow as tf\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl-GcyGXMBJ9"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7z8AJjW2MDHx"
      },
      "source": [
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead, top_k_top_p_filtering\n",
        "from transformers import TFAutoModelWithLMHead, tf_top_k_top_p_filtering\n",
        "\n",
        "from pprint import pprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufsiNBKUM81e"
      },
      "source": [
        "## Masked Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1r_dReC8M_Q8"
      },
      "source": [
        "Masked language modeling is the task of masking tokens in a sequence with a masking token, and prompting the model to fill that mask with an appropriate token. This allows the model to attend to both the right context (tokens on the right of the mask) and the left context (tokens on the left of the mask). Such a training creates a strong basis for downstream tasks requiring bi-directional context, such as SQuAD (question answering, see [Lewis, Lui, Goyal et al.](https://arxiv.org/abs/1910.13461), part 4.2). \n",
        "\n",
        "Here is an example of using pipelines to replace a mask from a sequence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IYOeyNWNEHJ"
      },
      "source": [
        "nlp = pipeline(\"fill-mask\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3God-mH6lqo"
      },
      "source": [
        "This outputs the sequences with the mask filled, the confidence score, and the token id in the tokenizer vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PCTygTD6fHq",
        "outputId": "a3b8cfa1-e17a-49e0-efea-ca7ddb2c9eec"
      },
      "source": [
        "pprint(nlp(f\"HuggingFace is creating a {nlp.tokenizer.mask_token} that the community uses to solve NLP tasks.\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'score': 0.17927460372447968,\n",
            "  'sequence': 'HuggingFace is creating a tool that the community uses to solve '\n",
            "              'NLP tasks.',\n",
            "  'token': 3944,\n",
            "  'token_str': ' tool'},\n",
            " {'score': 0.1134939044713974,\n",
            "  'sequence': 'HuggingFace is creating a framework that the community uses to '\n",
            "              'solve NLP tasks.',\n",
            "  'token': 7208,\n",
            "  'token_str': ' framework'},\n",
            " {'score': 0.05243545398116112,\n",
            "  'sequence': 'HuggingFace is creating a library that the community uses to '\n",
            "              'solve NLP tasks.',\n",
            "  'token': 5560,\n",
            "  'token_str': ' library'},\n",
            " {'score': 0.03493543714284897,\n",
            "  'sequence': 'HuggingFace is creating a database that the community uses to '\n",
            "              'solve NLP tasks.',\n",
            "  'token': 8503,\n",
            "  'token_str': ' database'},\n",
            " {'score': 0.02860247902572155,\n",
            "  'sequence': 'HuggingFace is creating a prototype that the community uses to '\n",
            "              'solve NLP tasks.',\n",
            "  'token': 17715,\n",
            "  'token_str': ' prototype'}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjG36GE2OVcR"
      },
      "source": [
        "Here is an example of doing masked language modeling using a model and a tokenizer. The process is the following:\n",
        "\n",
        "1. Instantiate a tokenizer and a model from the checkpoint name. The model is identified as a BERT model and loads it with the weights stored in the checkpoint.\n",
        "2. Define a text and a few questions.\n",
        "3. Iterate over the questions and build a sequence from the text and the current question, with the correct model-specific separators token type ids and attention masks.\n",
        "4. Pass this sequence through the model. This outputs a range of scores across the entire sequence tokens (question and text), for both the start and end positions.\n",
        "5. Compute the softmax of the result to get probabilities over the tokens.\n",
        "6. Fetch the tokens from the identified start and stop values, convert those tokens to a string.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lnhjx-EaOUsU"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "model = TFAutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "\n",
        "text = r\"\"\"\n",
        " ðŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
        " architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural\n",
        " Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
        " TensorFlow 2.0 and PyTorch.\n",
        "\"\"\"\n",
        "\n",
        "questions = [\n",
        "  \"How many pretrained models are available in ðŸ¤— Transformers?\",\n",
        "  \"What does ðŸ¤— Transformers provide?\",\n",
        "  \"ðŸ¤— Transformers provides interoperability between which frameworks?\"\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxFQh5tk-25B",
        "outputId": "10307506-8fa6-46c2-b27f-1489cc463478"
      },
      "source": [
        "for question in questions:\n",
        "  inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"tf\")\n",
        "  input_ids = inputs[\"input_ids\"].numpy()[0]\n",
        "\n",
        "  outputs = model(inputs)\n",
        "  answer_start_scores = outputs.start_logits\n",
        "  answer_end_scores = outputs.end_logits\n",
        "\n",
        "  # Get the most likely beginning of answer with the argmax of the score\n",
        "  answer_start = tf.argmax(answer_start_scores, axis=1).numpy()[0]\n",
        "  # Get the most likely end of answer with the argmax of the score\n",
        "  answer_end = (tf.argmax(answer_end_scores, axis=1) + 1).numpy()[0]\n",
        "\n",
        "  answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start: answer_end]))\n",
        "\n",
        "  print(f\"Question: {question}\")\n",
        "  print(f\"Answer: {answer}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: How many pretrained models are available in ðŸ¤— Transformers?\n",
            "Answer: over 32 +\n",
            "Question: What does ðŸ¤— Transformers provide?\n",
            "Answer: general - purpose architectures\n",
            "Question: ðŸ¤— Transformers provides interoperability between which frameworks?\n",
            "Answer: tensorflow 2. 0 and pytorch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-tdLBZCCveP"
      },
      "source": [
        "## PyTorch implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO5_CIA2Rv31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "7ca54a165f4f4ed79aad5d22ef96fb77",
            "629b4d7a6be34ae78460ac2f32480d24",
            "b6bd0d06bb3e46dc913f7b065bb851a9",
            "4b6bdf495be941ca9acfdedad9fa7208",
            "d7fb7f4be56a44328cba826be711cf76",
            "8f8d6403f0634504a86002bb99147c4e",
            "cb15721da81c44228ef606e212209373",
            "1afcee0e45514679be479275b8e38423"
          ]
        },
        "outputId": "68926017-f1d0-4e1e-8bbd-2ea9ed29a2c5"
      },
      "source": [
        "# Pytorch\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ca54a165f4f4ed79aad5d22ef96fb77",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1340675298.0, style=ProgressStyle(descrâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Km0Y33QSL2X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07c1c3cf-b6e7-4e8d-bfe2-e4892f8aa505"
      },
      "source": [
        "for question in questions:\n",
        "  inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
        "  input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "\n",
        "  outputs = model(**inputs)\n",
        "  answer_start_scores = outputs.start_logits\n",
        "  answer_end_scores = outputs.end_logits\n",
        "\n",
        "  # Get the most likely beginning of answer with the argmax of the score\n",
        "  answer_start = torch.argmax(answer_start_scores)\n",
        "  # Get the most likely end of answer with the argmax of the score\n",
        "  answer_end = torch.argmax(answer_end_scores) + 1\n",
        "\n",
        "  answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start: answer_end]))\n",
        "\n",
        "  print(f\"Question: {question}\")\n",
        "  print(f\"Answer: {answer}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: How many pretrained models are available in ðŸ¤— Transformers?\n",
            "Answer: over 32 +\n",
            "Question: What does ðŸ¤— Transformers provide?\n",
            "Answer: general - purpose architectures\n",
            "Question: ðŸ¤— Transformers provides interoperability between which frameworks?\n",
            "Answer: tensorflow 2. 0 and pytorch\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}